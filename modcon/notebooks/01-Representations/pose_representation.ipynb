{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right\">\n",
    "  <img src=\"../images/dtlogo.png\" alt=\"Logo\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’» 01 - State representation, coordinate systems, and pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *state* describes the condition of our robot at a specific time. This may include information about where the robot is, or other factors related to the environment that may affect the robot. When we develop a robotics algorithm, we are responsible to decide the information that we wish to include in the state. In other words, the definition of state depends on the task that we are trying to solve. Ideally, we want a state to exhibit the following properties:\n",
    "\n",
    "1. Markov property: future state is independent of the past given the present:\n",
    "\n",
    "\n",
    "$$x_{t+1} = f(x_t, x_{t-1}, \\dots, x_0; u_t, \\dots, u_0) = f(x_t; u_t)$$\n",
    "\n",
    "\n",
    "2. A minimally sufficient statistic for the task (i.e., it contains only the information that we need to solve the task)\n",
    "3. Permits efficient computation\n",
    "4. Generalizable\n",
    "\n",
    "An example of a state representation that is often used in robotics is the *pose*. A *pose* represents the location of the robot in the world as well as its orientation (i.e., what direction is the robot facing towards). Since we live in a 3-dimensional (3D) world, we represent the location of the robot as its $(x,y,z)$ coordinate in the world. We can represent orientation in terms of its roll, pitch, and yaw angles (also known as Euler angle representation). For example, the yaw angle $\\theta_{yaw}$ will be the angle difference between the longitudinal axis of the robot from the reference frame.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/representations/roll_pitch_yaw.png\">\n",
    "  <figcaption>Illustration of roll, pitch, and yaw (source: https://en.wikipedia.org/wiki/Aircraft_principal_axes).</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that the location and the orientation of the robot can be defined either in the global coordinate frame (i.e., relative to where the origin is) or relative to any other coordinate frames. Combining location and orientation, we can write the pose $q$ in a vector/matrix form:\n",
    "\n",
    "$$\n",
    "q = \n",
    "\\begin{bmatrix}\n",
    "x & y & z & \\theta_{roll} & \\theta_{pitch} & \\theta_{yaw}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The pose $q$ can also be represented in the form of a homogeneous transformation matrix, sinces poses are members of the Matrix Lie Group called the **S**pecial **E**uclidean group $SE(3)$:\n",
    "\n",
    "$$\n",
    "SE(3) = \\{T = \n",
    "\\begin{bmatrix}\n",
    "R & r \\\\\n",
    "0^T & 1\n",
    "\\end{bmatrix}\n",
    "\\in \\mathrm{R}^{4 \\times 4} | R \\in SO(3), r \\in \\mathrm{R}^{3}\n",
    "\\},\n",
    "$$\n",
    "\n",
    "where $R$ denotes the rotation matrix, and $r$ denotes the translation component. This matrix $T$ also has a nice property where \n",
    "\n",
    "$$\n",
    "T^{-1} = \n",
    "\\begin{bmatrix}\n",
    "R^T & -R^Tr \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "One of the reasons to represent the pose in this form is because it allows us to conveniently move between frames (see later in this activity). \n",
    "\n",
    "Although we have been discussing the pose in $SE(3)$, since we are using a mobile robot and we assume the world is flat, we can simplify the pose of our robot to only its $(x,y)$ coordinate and its yaw (or orientation) angle $\\theta$. This is because, in addition to the assumption that the robot is always located at a constant elevation, we can also safely assume roll and pitch angles to always be constant since the robot always touches the ground. So in this case, the pose is simply $\n",
    "q = \n",
    "\\begin{bmatrix}\n",
    "x & y & \\theta\n",
    "\\end{bmatrix}\n",
    "$, which we can also write in $SE(2)$:\n",
    "\n",
    "$$\n",
    "SE(2) = \\{T = \n",
    "\\begin{bmatrix}\n",
    "R & r \\\\\n",
    "0^T & 1\n",
    "\\end{bmatrix}\n",
    "\\in \\mathrm{R}^{3 \\times 3} | R \\in SO(2), r \\in \\mathrm{R}^{2}\n",
    "\\},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\small\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\\n",
    "r = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$$\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & x \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's first make sure that we understand how to represent a pose by doing the exercise below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE: representing pose in $SE(n)$**\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/representations/pose_exercise.png\">\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "**Question:**\n",
    "\n",
    "The figure above illustrates the location of the robot (the orange dot) and its orientation, where the angle is measured from the $x$-axis (assume the angle to have a positive measure if the rotation is counterclockwise). How do we write the pose in $SE(2)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as  plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: type your answer here\n",
    "\n",
    "p = np.array([\n",
    "    [None, None, None],\n",
    "    [None, None, None],\n",
    "    [None, None, None]\n",
    "])\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can find the solutions to this and other questions in the [solutions file](../01-Representations/SOLUTIONS-pose_representation.ipynb) in this folder. Give it a honest shot yourself before peeking!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II: Moving between frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to convert a pose from one frame to another since we may have multiple coordinate frames (e.g., due to the use of multiple cameras and other sensors). As mentioned in the previous section, representing the pose in $SE(n)$ allows us to conveniently move between frames. We can do this by multiplying these transformation matrices together. For example, consider two different poses $a$ and $b$. If we know the pose $a$ in the origin frame $o$ (i.e., $p^o_a$) and the pose $b$ in the frame of $a$ (i.e., $p^a_b$), we can then compute the pose $b$ relative to the origin frame $o$ using the following relation:\n",
    "\n",
    "$$p^o_b = p^o_a \\cdot p^a_b,$$\n",
    "\n",
    "where $p^o_b$, $p^o_a$, and $p^a_b$ are represented in $SE(n)$. Similarly, if we are given $p^a_o$ and $p^o_b$, we can compute $p^a_b$ via:\n",
    "\n",
    "$$p^a_b = p^a_o \\cdot p^o_b$$\n",
    "\n",
    "Generally, we can combine these transformations together like the following:\n",
    "\n",
    "$$p^\\color{red}{a}_\\color{blue}{f} = p^\\color{red}{a}_\\color{green}{b} \\cdot p^\\color{green}{b}_\\color{orange}{c} \\cdot p^\\color{orange}{c}_\\color{purple}{d} \\cdot p^\\color{purple}{d}_\\color{gray}{e} \\cdot p^\\color{gray}{e}_\\color{blue}{f}$$\n",
    "\n",
    "In addition, another handy relation to know is that $p^a_b = (p^b_a)^{-1}$. \n",
    "\n",
    "Let's now take a look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE: moving between frames (from robot frame to map frame)**\n",
    "\n",
    "You are the head of operations, and you gave to your Duckiebot a critical recognition mission: identify and place road obstacles on the map. During its mission, your Duckiebot has GPS support, and it knows its own position in the global (map) frame. It encounters its first obstacle at position $x = 2$m and $y = 0.4$m, and orientation $\\theta$ = 110 degrees. The obstacle itself is at 30cm at 50 degrees (counter-clockwise) from the Duckiebot. Where is the location of the obstacle in the map (i.e., global) frame?\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/representations/moving_frame_exercise_1.png\">\n",
    "  <figcaption>Illustration of the example problem. Here, the orange and blue dots represent the robot and the obstacle, respectively. Our task is to determine the position of the blue dot in the global frame.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "<!-- We denote our origin frame as $o$, ... as $a$, and ... as $b$. -->\n",
    "\n",
    "<!-- Compute $p^o_a$ and $p^b_a$, and use them to compute $p^a_b$.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to initialize the problem\n",
    "\n",
    "duckie_pos_g = np.array([2, 0.4])   # Position of Duckiebot in map/global frame\n",
    "duckie_or_g = 110                   # Orientation of Duckiebot in map/global frame (in degrees)\n",
    "obstacle_dist_to_duckie = 0.3       # Obstacle distance to the Duckiebot\n",
    "obstacle_angle = 50                 # Obstacle angle with respect to Duckiebot (in degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code here to compute the answer\n",
    "\n",
    "\n",
    "# TODO: put your answer here instead of None, None (position of obstacle in global frame)\n",
    "obstacle_pos_g = np.array([None, None])    \n",
    "print(obstacle_pos_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct result: [1.71809221 0.50260604]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE: moving between frames (from global frame to robot frame)**\n",
    "\n",
    "A concerned Duckie citizen calls you: \"I can see a piece of roof on the road! It's positioned at $x = 4$m and $y = - 1$m!\" This is valuable information but, to add it to your map, you need your Duckiebot to verify it. Luckily, your Duckiebot is closeby, at $x = 3.5$m, $y = -1.2$m, and oriented at $\\theta = 45$ degrees. In the robot frame, what are the coordinates of the obstacle described by the concerned Duckie citizen?\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../images/representations/moving_frame_exercise_2.png\">\n",
    "  <figcaption>Same as before, the orange and blue dots represent the robot and the obstacle, respectively. But our task is now to determine the position of the blue dot in the robot frame.</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this cell to initialize the problem\n",
    "\n",
    "duckie_pos_g = np.array([3.5, -1.2])   # Position of Duckiebot in global frame\n",
    "duckie_or_g = 45                       # Orientation of Duckiebot in global frame (degrees)\n",
    "obstacle_pos_g = np.array([4, -1])     # Position of obstacle in global frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your code here to compute the answer\n",
    "\n",
    "\n",
    "# TODO: put your answer here instead of None, None (position of obstacle in global frame)\n",
    "obstacle_pos_r = np.array([None, None])    \n",
    "print(obstacle_pos_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct result: [ 0.49497475 -0.21213203]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now proceed to the [wheel calibration tutorial](../02-Wheel-Calibration/wheels_calibration.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Credit: [Rey Reza Wiyatno](https://github.com/rrwiyatn)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
