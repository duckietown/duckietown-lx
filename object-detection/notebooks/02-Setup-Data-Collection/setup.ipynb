{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "<img src=\"../../assets/images/dtlogo.png\" alt=\"Duckietown\" width=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Machine-learned object detection models can be extremely useful. They are faster and often more reliable than traditional computer vision models. Additionally, we can use pretrained model weights to cut down immensely on training time.\n",
    "\n",
    "Here's an example of what an object detector might output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"500\"\n",
    "src=\"https://www.youtube.com/embed/3jD02dxL6gg\" \n",
    "frameborder=\"0\" \n",
    "allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \n",
    "allowfullscreen\n",
    "style=\"margin: auto; display: block\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will create your own Duckietown object detection dataset. You will learn about the general structure such a dataset should follow. You will train the object detection model on that dataset ([in a subsequent notebook](../03-Training/training.ipynb). Finally, you will integrate the model into a ROS node and test the integration ([in the last notebook](../04-Integration/integration.ipynb), so that your Duckiebot knows how to recognize duckie pedestrians (and thus avoid them). You can test your object detector in simulation and on your real Duckiebot.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Setup  \n",
    "2. Investigation\n",
    "3. Data collection\n",
    "4. Training\n",
    "5. Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we need some global variables. These allow you to change the directory where we store the data you will need. You can also change the image size to reflect what your final model uses, but you can worry about that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"/code/object-detection/assets/duckietown_object_detection_dataset\"\n",
    "IMAGE_SIZE = 416\n",
    "# this is the percentage of real data that will go into the training set (as opposed to the testing set)\n",
    "REAL_TRAIN_TEST_SPLIT_PERCENTAGE = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "While you will build your own dataset with simulated images in part 2, it would be unreasonable to ask you to build your own dataset of real images. Run the cell below to download a dataset of pre-labelled real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import runp\n",
    "\n",
    "# download dataset\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    runp(f\"rm -rf {DATASET_DIR}/*\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/labels\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/train/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/train/labels\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/val/images\")\n",
    "    runp(f\"mkdir -p {DATASET_DIR}/val/labels\")\n",
    "else:\n",
    "    print(\"Folder structure already exists!\")\n",
    "\n",
    "# download dataset\n",
    "if not (len(os.listdir(f\"{DATASET_DIR}/images\"))):\n",
    "    !wget -O /tmp/dataset.zip https://duckietown-public-storage.s3.amazonaws.com/assets/mooc/2022/duckietown_object_detection_dataset.zip\n",
    "    runp(f\"unzip -q /tmp/dataset.zip -d $(dirname {DATASET_DIR})\")\n",
    "    runp(f\"rm /tmp/dataset.zip\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These real-world images are not the right size. Run the cell bellow to resize them (and resize the associated bounding boxes accordingly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.utils import xminyminxmaxymax2xywfnormalized, train_test_split, makedirs, runp\n",
    "\n",
    "with open(f\"{DATASET_DIR}/annotation/final_anns.json\") as anns:\n",
    "    annotations = json.load(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_index = 0\n",
    "\n",
    "all_image_names = []\n",
    "    \n",
    "def save_img(img, boxes, classes):\n",
    "    global npz_index\n",
    "    cv2.imwrite(f\"{DATASET_DIR}/images/real_{npz_index}.jpg\", img)\n",
    "    with open(f\"{DATASET_DIR}/labels/real_{npz_index}.txt\", \"w\") as f:\n",
    "        for i in range(len(boxes)):\n",
    "            f.write(f\"{classes[i]} \"+\" \".join(map(str,boxes[i]))+\"\\n\")\n",
    "    npz_index += 1\n",
    "    all_image_names.append(f\"real_{npz_index}\")\n",
    "\n",
    "filenames = tqdm(os.listdir(f\"{DATASET_DIR}/frames\"))\n",
    "for filename in filenames:\n",
    "    img = cv2.imread(f\"{DATASET_DIR}/frames/{filename}\")\n",
    "\n",
    "    orig_y, orig_x = img.shape[0], img.shape[1]\n",
    "    scale_y, scale_x = IMAGE_SIZE/orig_y, IMAGE_SIZE/orig_x\n",
    "\n",
    "    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    if filename not in annotations:\n",
    "        continue\n",
    "\n",
    "    for detection in annotations[filename]:\n",
    "        box = detection[\"bbox\"]\n",
    "        label = detection[\"cat_name\"]\n",
    "\n",
    "        if label not in [\"duckie\", \"cone\"]:\n",
    "            continue\n",
    "\n",
    "        orig_x_min, orig_y_min, orig_w, orig_h = box\n",
    "\n",
    "        x_min = int(np.round(orig_x_min * scale_x))\n",
    "        y_min = int(np.round(orig_y_min * scale_y))\n",
    "        x_max = x_min + int(np.round(orig_w * scale_x))\n",
    "        y_max = y_min + int(np.round(orig_h * scale_y))\n",
    "\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        classes.append(1 if label == \"duckie\" else 2)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "\n",
    "    boxes = np.array([xminyminxmaxymax2xywfnormalized(box, IMAGE_SIZE) for box in boxes])\n",
    "    classes = np.array(classes)-1\n",
    "    \n",
    "    save_img(img, boxes, classes)\n",
    "\n",
    "\n",
    "\n",
    "train_test_split(all_image_names, REAL_TRAIN_TEST_SPLIT_PERCENTAGE, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, you're all set! We'll explain how the code above worked as you continue through this notebook.\n",
    "\n",
    "## 2. Investigation\n",
    "\n",
    "What does an object detection dataset look like? Clearly, the specifics will depend on the convention used by specific models, but the general idea is intuitive:\n",
    "\n",
    "- We need an image\n",
    "- This image might have many bounding boxes in it, so we need some sort of list of coordinates\n",
    "- These bounding boxes must be associated with a class\n",
    "\n",
    "How are the bounding boxes defined?\n",
    "\n",
    "![image of a bounding box](../../assets/images/bbox.png)\n",
    "\n",
    "Some conventions use `x_min y_min width height`, whereas others use `x_min y_min x_max y_max`, and others use `x_center y_center width height`. In this exercise, the model we recommend ([YoloV5](https://github.com/Velythyl/yolov5)) uses `x_center y_center width height`.\n",
    "\n",
    "And how do we actually obtain these bounding boxes? In real-life applications, you would need to label a dataset of images by hand. But if you have access to a simulator that is able to segment images, you could obtain the bounding boxes directly from the segmented images. \n",
    "\n",
    "If you take a look at Pytorch's object detection [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), that is similar to what they do. While their images were segmented by hand, the tutorial uses the same technique that we will use here to obtain the bounding boxes. Their images look like this:\n",
    "\n",
    "![image with bounding boxes](../../assets/images/FudanPed.png)\n",
    "<p align=\"center\">\\[Source: https://www.cis.upenn.edu/~jshi/ped\\_html/\\]</p>\n",
    "\n",
    "And they simply calculate the min and max x and y coordinates of the segmented objects to obtain the bounding box.\n",
    "\n",
    "We will use the segmented mode in the Duckietown simulator to compute the bounding boxes of non-segmented images.\n",
    "\n",
    "#### What we want to detect\n",
    "\n",
    "The goal of this exercise is to make Duckietown safer: we want to be able to detect duckie pedestrians on the road and avoid squishing them. We also want to detect trucks, buses, and cones. Here is the complete list, along with their corresponding IDs:\n",
    "\n",
    "0. Duckie\n",
    "1. Cone\n",
    "2. Truck\n",
    "3. Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data collection\n",
    "\n",
    "\n",
    "### Format\n",
    "\n",
    "We are going to supplement the data from the real dataset that we already downloaded with data automatically generated from the simulator. \n",
    "\n",
    "The script we will use for this is the [data_collection.py](../../packages/utils/data_collection.py) file. \n",
    "You will need to edit it in order to change the number of images generated, the map used by the simulator to generate images, and other parameters. More instructions on that later in the notebook. \n",
    "\n",
    "The purpose of the [data_collection.py](../../packages/utils/data_collection.py) script is to automatically generate data for you from the simulator. \n",
    "In the rest of this activity we will walk step by step through the process. \n",
    "\n",
    "Of course, your dataset's format depends heavily on your model. If you want to use the [YoloV5](https://github.com/duckietown/yolov5) model that we suggest, you should closely follow their [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data).\n",
    "\n",
    "Your data should follow the following directory structure:\n",
    "\n",
    "![image of dataset save format](../../assets/images/dataset_format.png)\n",
    "\n",
    "The dataset is called `duckietown_object_detection_dataset` and is stored inside the `assets/` directory of this learning experience.\n",
    "We have created two subdirectories in that folder: `train` and `val`. Both these directories should contain two subdirectories, `images` and `labels`. Inside `images`, you must place your images, and inside `labels`, you must place the images' bounding boxes data. Notice that the label files use the same name as their corresponding image files but with a different extension. In other words, the data for `0.jpg` can be found in `0.txt`.\n",
    "\n",
    "The format for the label files is fairly simple. For each bounding box in the corresponding image, write a row of the form `class x_center y_center width height`. Keep in mind that the pixel data must be 0-to-1 normalized (i.e., you can calculate the usual `x_center y_center width height` in pixel space and divide by the image's size). For example,\n",
    "\n",
    "    0 0.5 0.5 0.2 0.2\n",
    "    1 0.60 0.70 0.4 0.2\n",
    "\n",
    "this says \"there is a duckie (class 0) centered in the image, whose width and height are 20% of the image's. There is also a cone (class 1) whose center is at 60% of the image's maximal x value and 70% of the image's maximal y value, and its width is 40% of the image's own while its height is 20%.\"\n",
    "\n",
    "Again, it is recommended that you read the guide posted on YoloV5's GitHub: [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data collection\n",
    "\n",
    "After you're done editing the [data_collection.py](../../packages/utils/data_collection.py) file, we will need to run it against the simulator.\n",
    "We will do that through this activity's virtual desktop environment.\n",
    "\n",
    "Access this activity's virtual desktop (also known and referred to as `VNC`) by running the following command from the root directory of this activity,\n",
    "\n",
    "```shell\n",
    "dts code workbench --simulation\n",
    "```\n",
    "\n",
    "Click on the URL that you see on screen to open VNC in your browser. Click the \"Data Collection\" icon on the desktop. \n",
    "This will run your [data_collection.py](../../packages/utils/data_collection.py) script file. \n",
    "If you edit the script in this editor, you need to close the application and click on the icon again for the changes to have an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data\n",
    "\n",
    "#### 1. Take the segmented image (this is provided to you by the simulator's rendering engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "mapping = {\n",
    "    \"house\": \"3deb34\",\n",
    "    \"bus\": \"ebd334\",\n",
    "    \"truck\": \"961fad\",\n",
    "    \"duckie\": \"cfa923\",\n",
    "    \"cone\": \"ffa600\",\n",
    "    \"floor\": \"000000\",\n",
    "    \"grass\": \"000000\",\n",
    "    \"barrier\": \"000099\"\n",
    "}\n",
    "mapping = {\n",
    "    key:\n",
    "        [int(h[i:i+2], 16) for i in (0,2,4)]\n",
    "    for key, h in mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with a few other files in the images folder. All of the original/segmented pairs are labeled as *_not_seg and *_seg\n",
    "obs = np.asarray(Image.open('../../assets/images/duckie_not_seg.png'))\n",
    "obs_seg = np.asarray(Image.open('../../assets/images/duckie_seg.png'))\n",
    "# define the mapping from objects to colours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove colors we are not interested in\n",
    "\n",
    "The function below removes all colors that do not match the given class name.\n",
    "We use this to isolate the objects of interest by isolating their respective color first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import segmented_image_one_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out by removing everything that is not a duckie in the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckie_masked_image = segmented_image_one_class(np.asarray(obs_seg),\"duckie\")\n",
    "plt.imshow(duckie_masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Find bounding boxes around each unique instance within the image\n",
    "\n",
    "The function below isolates the object by finding the contours of the colored blob in the image above.\n",
    "\n",
    "This results in the bounding box around the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import find_all_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes the original image and computed bounding boxes and superimposes the bounding boxes to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes(img, boxes):\n",
    "    import matplotlib.patches as patches\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box[0], box[2]), box[1]-box[0], box[3]-box[2], linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions out on the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = find_all_bboxes(duckie_masked_image)\n",
    "show_image_with_boxes(obs,boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Let's do that but for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.setup_activity import find_all_boxes_and_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes, all_classes = find_all_boxes_and_classes(obs_seg)\n",
    "show_image_with_boxes(obs, all_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will need to save the non-segmented version of the image, and write its bounding boxes + their classes to a corresponding txt file. This is already implemented in the [data_collection.py](../../packages/utils/data_collection.py) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining with the real dataset & training/test set splits\n",
    "\n",
    "When training supervised learning models, one must worry about overfitting to the training set. If you can keep *some* of your dataset *out* of your training data, you can use it to verify that your model does not overfit to your dataset by *testing* it on the data you left out. We call this chunk of data the *validation set*. \n",
    "\n",
    "You can experiment with the `REAL_TRAIN_TEST_SPLIT_PERCENTAGE` variable defined at the top of this notebook. Tune its value to adjust the percentage of the **real** data that is used for training as opposed to testing. There is a similar variable defined in [data_collection.py](../../packages/utils/data_collection.py), called `SIMULATED_TRAIN_SPLIT_PERCENTAGE` which controls the percentage of the **simulated** data that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "\n",
    "You can continue with the [Training notebook](../03-Training/training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
