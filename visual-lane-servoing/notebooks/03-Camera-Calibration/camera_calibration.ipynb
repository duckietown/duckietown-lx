{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "<img src=\"../../assets/images/dtlogo.png\" alt=\"Duckietown\" width=\"50%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39bc08-95db-4668-abce-f1406321888d",
   "metadata": {},
   "source": [
    "# ðŸš™ 03 - Camera Calibration\n",
    "\n",
    "<!--\n",
    "ðŸ’» ðŸš™\n",
    "-->\n",
    "\n",
    "\n",
    "In this activity we will calibrate the parameters of the camera projection matrix for our robots. This involves two procedures:\n",
    "\n",
    "1. **Intrinsic Calibration**: Estimate the intrinsic parameters of your camera, including the focal length(s), center of projection, and lens distortion parameters (which we didn't cover in this course).\n",
    "2. **Extrinsic Calibration**: Estimate the homography that transforms points between the ground plane and the image plane.\n",
    "\n",
    "You will need the Duckietown hardware to proceed. If you do not have a Duckiebot and Duckietown, proceed to the next activity: the [image filtering tutorial](../04-Image-Filtering/image_filtering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c8550-2414-44ed-81df-e6e08fb9d9b1",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../../assets/images/pinhole_camera_model/pinhole-projection-b.png\", width=500px>\n",
    "  <p>Pinhole camera geometry.</p>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "As we have seen, the camera matrix models the transformation of 3D scene points, expressed relative to a Cartesian world frame, to their corresponding projection on the image plane, expressed in terms of pixels. Assuming that we express the points in homogeneous coordinates, we have shown that we can decompose the transformation as the product of two matrices:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_i &= \n",
    "\\begin{bmatrix}\n",
    "f_x & s & p_x\\\\\n",
    "0 & f_y & p_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{R} \\; \\vert \\; \\mathbf{t}\n",
    "\\end{bmatrix} \\mathbf{X}_i\\\\\n",
    "& = K \\begin{bmatrix}\n",
    "\\mathbf{R} \\; \\vert \\; \\mathbf{t}\n",
    "\\end{bmatrix} \\mathbf{X}_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}_i$ is the four-vector that specifies the homogeneous coordinates of a point in the world frame and $\\mathbf{x}_i$ is the three-vector that specifies the homogeneous coordinates in the image. \n",
    "\n",
    "In the following, we will estimate the parameters of these two matrices, starting with the camera's intrinsic matrix $K$.\n",
    "\n",
    "## Intrinsic Calibration\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../../assets/images/camera_calibration/checkerboard.jpg\", width=500px>\n",
    "  <p>The checkerboard used for calibration annotated with the detected points.</p>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Calibration involves optimizing some objective (e.g., reprojection error) given known correspondences between coordinates in the scene $\\mathbf{X}_i$ and their image-space projections $\\mathbf{x}_i$. Checkerboard patterns are often used for this purpose since they provide points that are easy to detect and well defined (i.e., we can easily define their coordinates). Because checkerboards are planar (i.e., $Z_i = 0 \\; \\forall i$) there is a homography that relates the coordinates of points on the checkerboard to their image-space coordinates.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_i &= \n",
    "\\begin{bmatrix}\n",
    "f_x & s & p_x\\\\\n",
    "0 & f_y & p_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R_{11} & R_{12} & R_{13} & t_x\\\\\n",
    "R_{21} & R_{22} & R_{23} & t_y\\\\\n",
    "R_{31} & R_{32} & R_{33} & t_z\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "X_i\\\\\n",
    "Y_i\\\\\n",
    "Z_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "f_x & s & p_x\\\\\n",
    "0 & f_y & p_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R_{11} & R_{12} & R_{13} & t_x\\\\\n",
    "R_{21} & R_{22} & R_{23} & t_y\\\\\n",
    "R_{31} & R_{32} & R_{33} & t_z\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "X_i\\\\\n",
    "Y_i\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix} \\quad \\textrm{since } Z_i = 0\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "f_x & s & p_x\\\\\n",
    "0 & f_y & p_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R_{11} & R_{12} & t_x\\\\\n",
    "R_{21} & R_{22} & t_y\\\\\n",
    "R_{31} & R_{32} & t_z\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "X_i\\\\\n",
    "Y_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "H_{11} & H_{12} & H_{13}\\\\\n",
    "H_{21} & H_{22} & H_{23}\\\\\n",
    "H_{31} & H_{32} & H_{33}\\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "X_i\\\\\n",
    "Y_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The vectors $\\mathbf{X}_i$ and $\\mathbf{x}_i$ specify the coordinates of the world and image points in *homogeneous coordinates*. \n",
    "\n",
    "Letting $\\mathbf{r}_1$ and $\\mathbf{r}_2$ be the first and second columns of the (unknown) rotation matrix associated with the world (checkerboard)-to-camera transformation, and $(u_i, v_i)$ be the pixel coordinates of point $\\mathbf{x}_i$ (i.e., the non-homogeneous coordinates), we have\n",
    "\n",
    "$$\n",
    "\\alpha\n",
    "\\begin{bmatrix}\n",
    "u_i\\\\\n",
    "v_i\\\\\n",
    "1\n",
    "\\end{bmatrix} = K\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{r}_1 & \\mathbf{r}_2 & \\mathbf{t}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X_i\\\\\n",
    "Y_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a scale term (since $\\mathbf{x}_i$ is in homogeneous coordinates). Letting $\\mathbf{h}_j$ be the $j^\\textrm{th}$ column of $H$, we can relate the above expression to the homography matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{h}_1 & \\mathbf{h}_2 & \\mathbf{h}_3\n",
    "\\end{bmatrix} = \\lambda K\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{r}_1 & \\mathbf{r}_2 & \\mathbf{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a non-zero constant (since the projection is only defined up-to-scale). \n",
    "\n",
    "As columns of the rotation matrix, $\\mathbf{r}_1$ and $\\mathbf{r}_2$ are orthonormal (i.e., they are orthogonal and they have unit norm). In other words, the columns $\\mathbf{r}_i$ of any rotation matrix $R$ exhibit the following properties\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_i^\\top \\mathbf{r}_j = \\left\\{\n",
    "\\begin{align}\n",
    "0 & \\quad i \\neq j\\\\\n",
    "1 & \\quad i=j\n",
    "\\end{align}\\right.\n",
    "$$\n",
    "\n",
    "In order to see why this is the case, let's start with the fact that the inverse of any rotation matrix is its transpose, i.e., $R^{-1} = R^\\top$. Let's consider two reference frames $A$ and $B$ that share the same origin. Let $R$ be the rotation matrix that transforms points expressed relative to frame $A$ in terms of their coordinates in frame $B$ (since the reference frames share the same origin, the transformation does not include a translation or, more precisely, the transformation is the zero-vector). Consider a vector $\\mathbf{X}$ that specifies the coordinates of a point with respect to frame $A$. We can express the coordinates for the same point with respect to frame $B$ as $\\mathbf{X}^\\prime = R \\mathbf{X}$. Similarly, we can transform $\\mathbf{X}^\\prime$, and any other vector in frame $B$, to frame $A$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{X} &= R^{-1}\\mathbf{X}^\\prime\\\\\n",
    "&= R^{-1}R\\mathbf{X}\\\\\n",
    "&= \\mathbf{X}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This, together with the fact that $R^{-1} = R^\\top$, implies that\n",
    "\n",
    "$$\n",
    "R^\\top R = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{r}_{1}^\\top \\mathbf{r}_1 & \\mathbf{r}_1^\\top \\mathbf{r}_2 & \\mathbf{r}_1^\\top \\mathbf{r}_3\\\\\n",
    "\\mathbf{r}_2^\\top \\mathbf{r}_1 & \\mathbf{r}_2^\\top \\mathbf{r}_2 & \\mathbf{r}_2^\\top \\mathbf{r}_3\\\\\n",
    "\\mathbf{r}_3^\\top \\mathbf{r}_1 & \\mathbf{r}_3^\\top \\mathbf{r}_2 & \\mathbf{r}_3^\\top \\mathbf{r}_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and, in turn, that the columns of any rotation matrix $R$ are orthonomral.\n",
    "\n",
    "\n",
    "One can show that we can exploit this property to get the following two constraints on $K$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}_1^\\top K^{-\\top} K^{-1}\\mathbf{h}_2 &= 0\\\\\n",
    "\\mathbf{h}_1^\\top K^{-\\top} K^{-1}\\mathbf{h}_1 &= \\mathbf{h}_2^\\top K^{-\\top}K^{-1}\\mathbf{h}_2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Having estimated the homography associated with a particular image of the checkerboard, the above equations constitute two constraints on the intrinsic parameters (homographies have eight degrees-of-freedom, while the transformation has six degrees-of-freedom, allowing us to only impose two constraints on the intrinsics from a particular image).\n",
    "\n",
    "We will estimate the intrinsic parameters of the matrix by collecting several images of the same checkerboard, either by moving the camera around in the scene or by keeping the camera fixed and changing the position and orientation of the checkerboard. There is a homography associated with each image of the checkerboard (since either the camera or checkerboard move between images, these homographies will be different for different images), and after estimating the homography, we can use the two equations above in computing the intrinsic matrix.\n",
    "\n",
    "For more information on this procedure, see the following paper, which proposed the idea:\n",
    "\n",
    "Zhengyou Zhang, [A Flexible New Technique for Camera Calibration](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf), Technical Report MSR-TR-98-71, Microsoft Research, 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faedd81-f811-46bb-ae23-4574369adacb",
   "metadata": {},
   "source": [
    "### ðŸš™ Intrinsic calibration procedure\n",
    "\n",
    "During this procedure we identify the instrinsic parameters of the camera. These are only a function of the camera specifications (lens, focus, pixel array, etc.), and not of the placement of the camera in the world. \n",
    "\n",
    "This procedure can be completed either by keeping the Duckiebot still and moving the calibration pattern, or viceversa. To ensure the calibration pattern is flat at all times, we suggest to keep the camera calibration pattern on the ground and move the Duckiebot.\n",
    "\n",
    "0. Make sure your Duckiebot is powered-on, its camera is working, and you have a camera calibration pattern available. Also, make sure to focus the camera by gently rotating the lens of the camera.\n",
    "\n",
    "    **Once the calibration is complete, do not touch the focus anymore, ever, as it will invalidate calibration.**\n",
    "\n",
    "1. Place your camera calibration pattern on a flat surface (e.g., a table, your Duckietown, or the floor). Make sure it stays flat on the ground and no wrinkles are present. Tape it down if needed. \n",
    "\n",
    "2. Open a terminal on your computer and type:\n",
    "\n",
    "        dts duckiebot calibrate_intrinsics ROBOTNAME\n",
    "\n",
    "\n",
    "3. When the window opens you will need to move the Duckiebot around (grab it with your hands and move it as you wish; it doesn't need to be on the ground.) so that is sees the calibration pattern.\n",
    "\n",
    "4. Move the Duckiebot in front of the pattern until you see colored lines overlaying the checkerboard. You will only see the colored lines if the entire checkerboard is within the field of view of the camera.\n",
    "\n",
    "    You should also see colored bars in the sidebar of the display window. These bars indicate the current range of the checkerboard in the cameraâ€™s field of view:\n",
    "    \n",
    "    * X bar: the observed horizontal range (left - right)\n",
    "    * Y bar: the observed vertical range (top - bottom)\n",
    "    * Size bar: the observed range in the checkerboard size (forward - backward from the camera direction)\n",
    "    * Skew bar: the relative tilt between the checkerboard and the camera direction\n",
    "\n",
    "    Move the robot such that the checkerboard is located at different locations, orientations, and scales in the image (note that you can alternatively keep the camera stationary and move the checkerboard instead, but if you do so make sure that you keep the checkerboard flat since, afterall, the whole procedure relies on the assumption that the target is planar). After each movement, make sure to pause long enough for the checkerboard to become highlighted. Once you have collected enough data, all four indicator bars will turn green. Press the `CALIBRATE` button in the sidebar.\n",
    "    \n",
    "    Calibration will take a few moments. Note that the screen may dim and / or become irresponsive. Donâ€™t worry, the calibration is working.\n",
    "\n",
    "<!--\n",
    "\n",
    "Congratulations, you found the alternative procedure! \n",
    "\n",
    "1. Place your powered-on Duckiebot on a table.\n",
    "\n",
    "2. Open a terminal on your computer and type:\n",
    "\n",
    "        dts duckiebot calibrate_intrinsics ROBOTNAME\n",
    "\n",
    "\n",
    "\n",
    "3. You have two options now:\n",
    "    - Stick the calibration pattern to a rigid but movable surface (e.g., a piece of cardboard) and move it in front of the camera while leaving the Duckiebot sitting on the table.\n",
    "    - Stick the calibration pattern to the floor with the checkerboard pattern facing up (tape it to the floor if it does not lay perfectly flat) and move your duckiebot up/down left/right with the camera facing towards the floor.\n",
    "\n",
    "\n",
    "4. Position the checkerboard in front of the camera until you see colored lines overlaying the checkerboard. You will only see the colored lines if the entire checkerboard is within the field of view of the camera.\n",
    "    You should also see colored bars in the sidebar of the display window. These bars indicate the current range of the checkerboard in the cameraâ€™s field of view:\n",
    "    * X bar: the observed horizontal range (left - right)\n",
    "    * Y bar: the observed vertical range (top - bottom)\n",
    "    * Size bar: the observed range in the checkerboard size (forward - backward from the camera direction)\n",
    "    * Skew bar: the relative tilt between the checkerboard and the camera direction\n",
    "    \n",
    "    Also, make sure to focus the image by rotating the mechanical focus ring on the lens of the camera.\n",
    "\n",
    "    **Do not touch the focus anymore, ever, as it will invalidate the calibration.**\n",
    "\n",
    "    With the checkerboard pattern on the ground, move the robot such that the checkerboard is located at different locations, orientations, and scales in the image. After each movement, make sure to pause long enough for the checkerboard to become highlighted. Once you have collected enough data, all four indicator bars will turn green. Press the `CALIBRATE` button in the sidebar.\n",
    "    \n",
    "    Calibration may take a few moments. Note that the screen may dim. Donâ€™t worry, the calibration is working.\n",
    "\n",
    "-->\n",
    "\n",
    "#### Save the calibration results\n",
    "\n",
    "Make sure to save your new intrinsic calibration parameters! You can save the results by pressing the `COMMIT` button in the side bar. (You never need to click the `SAVE` button.)\n",
    "\n",
    "\n",
    "#### Final check to make sure itâ€™s stored\n",
    "\n",
    "You can verify that the new gain value has been saved on your Duckiebot by opening the Dashboard > File Manager > config > calibrations > camera_intrinsic page. \n",
    "\n",
    "You should find a file named `ROBOTNAME.yaml` in addition to the `default.yaml`. Double click on it and verify that the gain value is indeed the one you chose. \n",
    "\n",
    "### Lens Distortion\n",
    "\n",
    "A pinhole camera is an idealized camera model. As discussed in lecture, we made the aperture sufficiently small that light reflecting off different locations in the scene impact different points on the image (sensor) plane. Here, the goal was to minimize the size of the \"circle of confusion\", regions of the image that record intensity from multiple points in the scene. However, by making the aperture so small, we are, by design, limiting the amount of light that reaches the sensor. This results in a low signal-to-noise ratio, meaning that it is difficult to tease apart valid signal from noise.\n",
    "\n",
    "In practice, cameras mitigate this effect by increasing the aperture to sizes that result in significant confusion (i.e., aliasing) and then placing a lens in front of the aperture to control light on the sensor. In this way, a greater amount of light is exposed to the sensor, while being focused on the corresponding pixels (i.e., reducing aliasing). \n",
    "\n",
    "However, lenses themselves are imperfect and cause some distortion in the image. There are several parametric models that describe the effects of different types of distortion and standard calibration procedures, including the one above, estimate these parameters along with the other intrinsic parameters of the camera.\n",
    "\n",
    "#### ðŸ’» Example: Exploring lens distortion in simulation\n",
    "\n",
    "In this example, we will visualize the ability to use the results of intrinsic calibration to remove distortions introduced by the lens. This procedure is often referred to as *rectification*.\n",
    "\n",
    "1. Open a terminal on your computer, and type \n",
    "\n",
    "       dts code build\n",
    "\n",
    "\n",
    "2. Wait for the build to finish, then type:\n",
    "\n",
    "       dts code workbench --sim\n",
    "\n",
    "\n",
    "3. Open VNC on you browser and click on the `VLS - Visual Lane Servoing Exercise` icon on your desktop. Minimize any windows that are opened as a result.\n",
    "\n",
    "\n",
    "4. Click on the `RQT Image View` icon on your desktop. This will bring up a window like the one below (though you likely won't see an image yet).\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../../assets/images/camera_calibration/rqt_image_view.png\", width=400px>\n",
    "  <p>RQT Image View GUI.</p>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "\n",
    "5. The `RQT Image View` GUI includes a drop-down menu at the top. If you click on this menu, you should see two entries\n",
    "\n",
    "    - `agent/camera_node/image/compressed`: The original (compressed) image from the Duckiebot's camera\n",
    "    - `agent/rectifier_node/image/compressed`: The image from the Duckiebot's camera after rectification\n",
    "    \n",
    "    \n",
    "    \n",
    "6. Compare the appearance of the two images, particularly as it pertains to lines in the scene.\n",
    "\n",
    "**Note**: If you don't see an image, try sending a single command to the Duckiebot using the `Joystick` GUI (there is an icon on the desktop) as well as clicking the refresh button to the right of the drop-down menu on the `RQT Image View` GUI.\n",
    "\n",
    "#### ðŸš™ Example: Exploring lens distortion on your Duckiebot\n",
    "\n",
    "In this example, we will visualize the ability to use the results of intrinsic calibration to remove distortions introduced by the lens. This procedure is often referred to as *rectification*.\n",
    "\n",
    "1. Open a terminal on your computer, and type \n",
    "\n",
    "       dts code build\n",
    "\n",
    "\n",
    "2. Wait for the build to finish, then type:\n",
    "\n",
    "       dts code workbench -b ROBOTNAME\n",
    "\n",
    "\n",
    "3. Place your Duckiebot in a lane facing in the direction of travel.\n",
    "\n",
    "\n",
    "4. Follow the same instructions at point 3 for the simulation-based example above. \n",
    "\n",
    "**Note**: Consider placing images with straight lines (e.g., the extrinsic calibration target mentioned below) in front of your Duckiebot's camera to see the effects of distortion.\n",
    "\n",
    "**Note**: If you don't see an image, try sending a single command to the Duckiebot using the `Joystick` GUI (there is an icon on the desktop) as well as clicking the refresh button to the right of the drop-down menu on the `RQT Image View` GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4db72-9818-48e8-9022-f09f0898ffce",
   "metadata": {},
   "source": [
    "## Extrinsic Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c372a80-86a9-4d0b-9790-edfc5256503a",
   "metadata": {},
   "source": [
    "Having estimated the intrinsic parameters of the camera, we are now ready to calulate the extrinsic parameters, namely the homography that relates points in a planar world frame to the image plane. In our case, the world plane will be the ground plane with the origin below the midpoint between the Duckiebot's drive wheels with the positive $x$-axis pointing forward and the positive $y$-axis pointing to the left. In this way, world frame moves with the robot.\n",
    "\n",
    "Much like the intrinsic calibration procedure, we estimate the homography using known correspondences between points expressed in the ground frame and the image, and we will again use a checkerboard pattern for this purpose. Unlike intrinsic calibration, however, we don't need to move the camera or target around since four point-pairs are sufficient given the eight degrees-of-freedom associated with the homography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f531981-c21a-41c6-a6bb-a2ee7688f577",
   "metadata": {},
   "source": [
    "### ðŸš™ Extrinsic calibration procedure\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"../../assets/images/camera_calibration/extrinsic-calibration-2.jpg\", width=500px>\n",
    "  <p>We calibrate the extrinsic parameters (homography) by placing the Duckiebot on a known checkerboard pattern.</p>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "1. Place your powered-on Duckiebot and checkerboard with the wheels aligned with the $y$-axis on the target and the vehicle centered laterally on the origin, as shown above. Try and keep the field-of-view of the camera as clutter-free as possible.\n",
    "\n",
    "2. Open a terminal on your computer and type:\n",
    "\n",
    "        dts duckiebot calibrate_extrinsics ROBOTNAME\n",
    "\n",
    "\n",
    "Follow the instructions on the screen. If successful, the calibration results will be automatically saved to your Duckiebot.\n",
    "\n",
    "\n",
    "#### Final check to make sure itâ€™s stored\n",
    "\n",
    "You can verify that the new gain value has been saved on your Duckiebot by opening the Dashboard > File Manager > config > calibrations > camera_extrinsic page. \n",
    "\n",
    "You should find a file named `ROBOTNAME.yaml` in addition to the `default.yaml`. Double click on it and verify that the gain value is indeed the one you chose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40150f0-59f7-4daf-ae43-696a721d5533",
   "metadata": {},
   "source": [
    "You can now move to the [image filtering tutorial](../04-Image-Filtering/image_filtering.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
